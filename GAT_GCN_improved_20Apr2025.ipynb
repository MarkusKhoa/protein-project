{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpevWvg0yTcU",
        "outputId": "1cb0a802-48ab-4534-f0f9-744ae2f1f41d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh0Hqu_Kfgra",
        "outputId": "50cbf204-5989-46e4-88ed-8d0c35654ae3"
      },
      "outputs": [],
      "source": [
        "!sudo apt install build-essential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LprvaujJyYDK",
        "outputId": "e9b2ce25-1f0e-4c74-b6da-765606a18614"
      },
      "outputs": [],
      "source": [
        "%pip install -r \"/content/drive/MyDrive/Protein-binding/requirements.txt\"\n",
        "%pip install datasets mdtraj dssp\n",
        "%pip install numpy --no-cache-dir\n",
        "%pip install pandas==2.2.0\n",
        "%pip install pykan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K64qv8WQyoHz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ast\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "import mdtraj as md\n",
        "import esm\n",
        "import gc\n",
        "import pickle\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_add_pool\n",
        "from Bio import SeqIO\n",
        "from Bio import PDB\n",
        "from Bio.PDB.DSSP import DSSP\n",
        "from Bio.PDB import Selection\n",
        "\n",
        "from Bio.PDB.Polypeptide import is_aa\n",
        "from Bio.SeqUtils import seq1\n",
        "from torch_geometric.data import Data\n",
        "from transformers import (AutoModelForTokenClassification, AutoTokenizer,\n",
        "                          AutoModelForMaskedLM, DataCollatorForTokenClassification,\n",
        "                           EsmForMaskedLM, EsmTokenizer, EsmModel, EsmForTokenClassification,\n",
        "                           TrainingArguments, Trainer, TrainerCallback\n",
        "                        )\n",
        "from kan import KAN\n",
        "from transformers.trainer_callback import ProgressCallback\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
        "                             matthews_corrcoef, roc_auc_score)\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pprint import pprint\n",
        "from datasets import Dataset\n",
        "from datetime import datetime\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
        "from glob import glob\n",
        "from loguru import logger\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Ignoring unrecognized record 'END'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ6OCJdb9ywD"
      },
      "source": [
        "### Preparing train-test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuCFy4Bb9xhm"
      },
      "outputs": [],
      "source": [
        "initial_train_df = pd.read_csv(\"/content/drive/MyDrive/Protein-binding/data/development_set/full_grouped_train_binding_sites_df.csv\")\n",
        "initial_train_df['binding_sites'] = initial_train_df['binding_sites'].apply(ast.literal_eval)\n",
        "initial_train_df['any_ligand_binding_sites'] = initial_train_df['any_ligand_binding_sites'].apply(ast.literal_eval)\n",
        "initial_train_df['metal_binding'] = initial_train_df['metal_binding'].apply(ast.literal_eval)\n",
        "initial_train_df['small_binding'] = initial_train_df['small_binding'].apply(ast.literal_eval)\n",
        "initial_train_df['nuclear_binding'] = initial_train_df['nuclear_binding'].apply(ast.literal_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_hAzEgI92hX"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "excluded_protein_id = ['Q9NZV6']\n",
        "\n",
        "train_df = initial_train_df[~initial_train_df['prot_id'].isin(excluded_protein_id)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgswJ0qw93Pf"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"/content/drive/MyDrive/Protein-binding/data/development_set/full_grouped_test_binding_sites_df.csv\")\n",
        "# test_df = pd.read_csv(\"/content/drive/MyDrive/Protein-binding/data/independent_set/grouped_test_46_new_binding_sites.csv\")\n",
        "\n",
        "test_df['binding_sites'] = test_df['binding_sites'].apply(ast.literal_eval)\n",
        "test_df['any_ligand_binding_sites'] = test_df['any_ligand_binding_sites'].apply(ast.literal_eval)\n",
        "test_df['metal_binding'] = test_df['metal_binding'].apply(ast.literal_eval)\n",
        "test_df['small_binding'] = test_df['small_binding'].apply(ast.literal_eval)\n",
        "test_df['nuclear_binding'] = test_df['nuclear_binding'].apply(ast.literal_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udz4ADny96mG"
      },
      "outputs": [],
      "source": [
        "# Initial sequences\n",
        "test_seq = test_df['sequence'].tolist()\n",
        "test_labels = test_df['metal_binding'].tolist() # Edit labels\n",
        "\n",
        "train_seq = train_df['sequence'].tolist()\n",
        "train_labels = train_df['metal_binding'].tolist() # Edit labels\n",
        "\n",
        "saved_model_name = '/content/drive/MyDrive/Protein-binding/trained_models/GCN_KAN_for_metal_binding_20Apr_model.pth'\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpMCt3zk45W8"
      },
      "source": [
        "### Tokenization and get embeddings from ESM-2 language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQn8Ir0O447i",
        "outputId": "ec4c13d5-1410-45c5-9111-5e2119f58afb"
      },
      "outputs": [],
      "source": [
        "pretrained_model = \"facebook/esm2_t33_650M_UR50D\"\n",
        "\n",
        "tokenizer = EsmTokenizer.from_pretrained(pretrained_model)\n",
        "max_sequence_length = 1000\n",
        "\n",
        "train_tokenized = tokenizer(train_seq, padding=True, truncation=True, max_length=max_sequence_length, return_tensors=\"pt\", is_split_into_words=False)\n",
        "# test_tokenized = tokenizer(test_seq, padding=True, truncation=True, max_length=max_sequence_length, return_tensors=\"pt\", is_split_into_words=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOJD385x9s8M"
      },
      "outputs": [],
      "source": [
        "def get_embeddings_list(tokenized_dataset, batch_size, model_name=\"facebook/esm2_t33_650M_UR50D\", device=\"cuda\",\n",
        "                       embedding_mode=\"multi_layer\", num_layers=4, return_hidden_states=True, return_attentions=False):\n",
        "    \"\"\"\n",
        "    Extract ESM-2 embeddings with multi-layer aggregation or attention-guided pooling.\n",
        "\n",
        "    Args:\n",
        "        tokenized_dataset: Dict with 'input_ids', 'attention_mask', and optionally 'sequence_id'.\n",
        "        batch_size: Number of sequences per batch.\n",
        "        model_name: Pretrained ESM-2 model name.\n",
        "        device: Device to run model on ('cuda' or 'cpu').\n",
        "        embedding_mode: 'multi_layer' for layer aggregation, 'attention_guided' for attention pooling.\n",
        "        num_layers: Number of layers to aggregate (for multi_layer mode).\n",
        "        return_hidden_states: Whether to return hidden states.\n",
        "        return_attentions: Whether to return attention weights (required for attention_guided).\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with sequence_id and per-residue embeddings.\n",
        "    \"\"\"\n",
        "    # Validate inputs\n",
        "    if embedding_mode == \"attention_guided\" and not return_attentions:\n",
        "        raise ValueError(\"Attention-guided pooling requires return_attentions=True\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = EsmModel.from_pretrained(model_name).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Extract input tensors\n",
        "    ids_list = tokenized_dataset['input_ids'].to(device)  # Shape: [num_sequences, max_length]\n",
        "    attention_mask_list = tokenized_dataset['attention_mask'].to(device)\n",
        "    sequence_ids = tokenized_dataset.get('sequence_id', list(range(len(ids_list))))\n",
        "\n",
        "    num_batches = (len(ids_list) + batch_size - 1) // batch_size\n",
        "    embeddings_list = []\n",
        "\n",
        "    for i in tqdm(range(num_batches), total=num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(ids_list))\n",
        "\n",
        "        batch_ids = ids_list[start_idx:end_idx]  # Shape: [batch_size, max_length]\n",
        "        batch_attention_mask = attention_mask_list[start_idx:end_idx]\n",
        "        batch_seq_ids = sequence_ids[start_idx:end_idx]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=batch_ids,\n",
        "                attention_mask=batch_attention_mask,\n",
        "                output_hidden_states=return_hidden_states,\n",
        "                output_attentions=(embedding_mode == \"attention_guided\")\n",
        "            )\n",
        "\n",
        "            if embedding_mode == \"multi_layer\":\n",
        "                # Aggregate last N layers\n",
        "                hidden_states = outputs.hidden_states[-num_layers:]  # List of (batch_size, seq_len, embedding_dim)\n",
        "                aggregated_embeddings = torch.stack(hidden_states, dim=0).mean(dim=0)  # Average across layers\n",
        "                # Alternative: Concatenate layers (uncomment to use)\n",
        "                # aggregated_embeddings = torch.cat(hidden_states, dim=-1)  # Shape: (batch_size, seq_len, num_layers * embedding_dim)\n",
        "\n",
        "                # Process each sequence\n",
        "                for j in range(aggregated_embeddings.shape[0]):\n",
        "                    mask = batch_attention_mask[j].bool()\n",
        "                    seq_embeddings = aggregated_embeddings[j][mask][1:-1]  # Remove <cls>, <eos>\n",
        "                    embeddings_list.append({\n",
        "                        'sequence_id': batch_seq_ids[j],\n",
        "                        'embeddings': seq_embeddings.cpu().numpy()\n",
        "                    })\n",
        "\n",
        "            elif embedding_mode == \"attention_guided\":\n",
        "                # Attention-guided pooling\n",
        "                hidden_states = outputs.hidden_states[-1]  # Last layer: (batch_size, seq_len, embedding_dim)\n",
        "                attentions = outputs.attentions[-1]  # Last layer attention: (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "                # Process each sequence individually to handle variable lengths\n",
        "                for j in range(hidden_states.shape[0]):\n",
        "                    mask = batch_attention_mask[j].bool()\n",
        "                    seq_hidden = hidden_states[j][mask]  # (valid_len, embedding_dim)\n",
        "                    seq_attention = attentions[j][:, mask][:, :, mask]  # (num_heads, valid_len, valid_len)\n",
        "                    # Average attention across heads\n",
        "                    attention_weights = seq_attention.mean(dim=0)  # (valid_len, valid_len)\n",
        "                    attention_weights = attention_weights / (attention_weights.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "                    # Weighted embedding\n",
        "                    weighted_embedding = torch.matmul(attention_weights, seq_hidden)  # (valid_len, embedding_dim)\n",
        "                    seq_embeddings = weighted_embedding[1:-1]  # Remove <cls>, <eos>\n",
        "                    embeddings_list.append({\n",
        "                        'sequence_id': batch_seq_ids[j],\n",
        "                        'embeddings': seq_embeddings.cpu().numpy()\n",
        "                    })\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"embedding_mode must be 'multi_layer' or 'attention_guided'\")\n",
        "\n",
        "    return embeddings_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbDLSnAFlrpO"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2_epdc8-ssC",
        "outputId": "e993a103-7177-440c-a107-20560916dfe2"
      },
      "outputs": [],
      "source": [
        "train_embeddings = get_embeddings_list(train_tokenized, batch_size = 8, model_name = pretrained_model, device= \"cuda\",\n",
        "                                       embedding_mode = \"multi_layer\", num_layers=2, return_hidden_states = True, return_attentions = False)\n",
        "\n",
        "# test_embeddings = get_embeddings_list(test_tokenized, batch_size = 8, model_name = pretrained_model, device = \"cuda\",\n",
        "#                                        embedding_mode = \"multi_layer\", num_layers=2, return_hidden_states = True, return_attentions = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uAzAtdQTLKG"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/Protein-binding/metal_train_embeddings.pkl', 'wb') as f:\n",
        "#        pickle.dump(train_embeddings, f)\n",
        "\n",
        "# with open('/content/drive/MyDrive/Protein-binding/test_embeddings.pkl', 'wb') as f:\n",
        "#     pickle.dump(test_embeddings, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ2TP3euTUg-"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/Protein-binding/train_embeddings.pkl', 'rb') as f:\n",
        "#        train_embeddings = pickle.load(f)\n",
        "\n",
        "# with open('/content/drive/MyDrive/Protein-binding/test_embeddings.pkl', 'rb') as f:\n",
        "#     test_embeddings = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIi3EfmI2-5C"
      },
      "source": [
        "### Features extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cxe0vMqb3iOY"
      },
      "outputs": [],
      "source": [
        "def get_structure(prot_id, pdb_file):\n",
        "    parser = PDB.PDBParser()\n",
        "    structure = parser.get_structure(prot_id, pdb_file)\n",
        "    return structure\n",
        "\n",
        "def extract_coordinates(structure):\n",
        "    # Extract Cα coordinates (central carbon atom)\n",
        "    coordinates = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                if \"CA\" in residue:  # Get Cα atom\n",
        "                    ca_atom = residue[\"CA\"]\n",
        "                    coord = ca_atom.get_coord()  # Returns numpy array [x, y, z]\n",
        "                    coordinates.append(coord)\n",
        "\n",
        "    return coordinates\n",
        "\n",
        "def get_amino_acid_types(structure):\n",
        "    amino_acids = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                # Get residue name (3-letter code)\n",
        "                if not is_aa(residue):\n",
        "                    continue\n",
        "                res_name = residue.get_resname()\n",
        "                # Convert to 1-letter code if needed\n",
        "                one_letter = seq1(res_name)\n",
        "                amino_acids.append(one_letter)\n",
        "    return amino_acids\n",
        "\n",
        "def get_secondary_structure_mdtraj(pdb_file, sequence_length):\n",
        "    \"\"\"\n",
        "    Extract secondary structure features from a .PDB file using mdtraj.\n",
        "\n",
        "    Args:\n",
        "        pdb_file (str): Path to the .PDB file.\n",
        "        sequence_length (int): Length of the protein sequence.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with:\n",
        "            - 'raw': List of raw secondary structure codes.\n",
        "            - 'one_hot': Tensor of one-hot encoded secondary structure.\n",
        "    \"\"\"\n",
        "    # Load the .PDB file with mdtraj\n",
        "    traj = md.load(pdb_file)\n",
        "\n",
        "    # Compute secondary structure\n",
        "    ss = md.compute_dssp(traj)[0]  # Returns codes like 'H', 'E', 'C', '-'('NA')\n",
        "\n",
        "    # One-hot encode\n",
        "    ss_onehot = []\n",
        "    for code in ss:\n",
        "        ss_onehot.append([\n",
        "            1 if code == 'H' else 0,\n",
        "            1 if code == 'E' else 0,\n",
        "            1 if code == 'T' else 0,\n",
        "            1 if code == 'C' or code == 'NA' else 0\n",
        "        ])\n",
        "\n",
        "    ss_onehot = torch.tensor(ss_onehot, dtype=torch.float32)\n",
        "\n",
        "    # Pad with zeros to match sequence length\n",
        "    if ss_onehot.shape[0] < sequence_length:\n",
        "        padding = torch.zeros((sequence_length - ss_onehot.shape[0], ss_onehot.shape[1]), dtype=torch.float32)\n",
        "        ss_onehot = torch.cat([ss_onehot, padding], dim=0)\n",
        "\n",
        "    return {\n",
        "        \"raw\": ss.tolist(),\n",
        "        \"one_hot\": ss_onehot\n",
        "    }\n",
        "\n",
        "def calculate_residue_distances(coordinates):\n",
        "    \"\"\"\n",
        "    Calculate pairwise distances between residues in a protein structure.\n",
        "    Args: Coordinates (list): List of residue atom's coordinates.\n",
        "    Returns:\n",
        "        np.ndarray: 2D array of pairwise distances.\n",
        "    \"\"\"\n",
        "    num_residues = len(coordinates)\n",
        "    distances = np.zeros((num_residues, num_residues))\n",
        "\n",
        "    for i in range(num_residues):\n",
        "        for j in range(i + 1, num_residues):\n",
        "            dist = np.linalg.norm(coordinates[i] - coordinates[j])\n",
        "            distances[i, j] = distances[j, i] = dist\n",
        "\n",
        "    return distances\n",
        "\n",
        "def get_dihedral_angles(pdb_file, sequence_length):\n",
        "    traj = md.load(pdb_file)\n",
        "    # Compute phi and psi angles\n",
        "    phi_indices, phi_angles = md.compute_phi(traj)\n",
        "    psi_indices, psi_angles = md.compute_psi(traj)\n",
        "\n",
        "    # Convert to degrees and create tensors\n",
        "    phi_angles = torch.tensor(np.degrees(phi_angles[0]), dtype=torch.float32).unsqueeze(1)  # Shape: [num_residues-1, 1]\n",
        "    psi_angles = torch.tensor(np.degrees(psi_angles[0]), dtype=torch.float32).unsqueeze(1)  # Shape: [num_residues-1, 1]\n",
        "\n",
        "    # Pad with zeros to match sequence length\n",
        "    while phi_angles.shape[0] < sequence_length:\n",
        "        phi_angles = torch.cat([phi_angles, torch.zeros(1, 1, dtype=torch.float32)], dim=0)\n",
        "    while psi_angles.shape[0] < sequence_length:\n",
        "        psi_angles = torch.cat([psi_angles, torch.zeros(1, 1, dtype=torch.float32)], dim=0)\n",
        "\n",
        "    return phi_angles, psi_angles\n",
        "\n",
        "\n",
        "def get_b_factors(structure, sequence_length):\n",
        "    b_factors = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                if \"CA\" in residue:\n",
        "                    ca_atom = residue[\"CA\"]\n",
        "                    b_factor = ca_atom.get_bfactor()\n",
        "                    b_factors.append(b_factor)\n",
        "\n",
        "    # Pad with zeros if b_factors length is less than sequence length\n",
        "    while len(b_factors) < sequence_length:\n",
        "        b_factors.append(0.0)\n",
        "\n",
        "    return torch.tensor(b_factors, dtype=torch.float32).unsqueeze(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvEqWK9Z3l-Y"
      },
      "outputs": [],
      "source": [
        "# Compute RSA values for amino acids\n",
        "MAX_SASA = {\n",
        "    'ALA': 129.0, 'ARG': 274.0, 'ASN': 195.0, 'ASP': 193.0, 'CYS': 167.0,\n",
        "    'GLU': 223.0, 'GLN': 225.0, 'GLY': 104.0, 'HIS': 224.0, 'ILE': 197.0,\n",
        "    'LEU': 201.0, 'LYS': 236.0, 'MET': 224.0, 'PHE': 240.0, 'PRO': 159.0,\n",
        "    'SER': 155.0, 'THR': 172.0, 'TRP': 285.0, 'TYR': 263.0, 'VAL': 174.0\n",
        "}\n",
        "\n",
        "def compute_rsa(structure_file, chain_id=None):\n",
        "    \"\"\"\n",
        "    Compute RSA for each residue in a protein structure.\n",
        "\n",
        "    Args:\n",
        "        structure_file (str): Path to the protein structure file (e.g., PDB file).\n",
        "        chain_id (str, optional): Chain ID to analyze (if None, uses the first chain).\n",
        "\n",
        "    Returns:\n",
        "        list: List of (residue_name, residue_id, rsa) tuples.\n",
        "    \"\"\"\n",
        "    traj = md.load(structure_file)\n",
        "\n",
        "    if chain_id:\n",
        "        chain = next(c for c in traj.topology.chains if c.chain_id == chain_id)\n",
        "        traj = traj.atom_slice([atom.index for atom in traj.topology.atoms if atom.residue.chain == chain])\n",
        "\n",
        "    sasa = md.shrake_rupley(traj, mode='residue')[0]  # Shape: [n_residues]\n",
        "    rsa_values = []\n",
        "    for i, residue in enumerate(traj.topology.residues):\n",
        "        if not residue.is_protein:\n",
        "            continue\n",
        "        res_name = residue.name\n",
        "        res_id = residue.resSeq\n",
        "        residue_sasa = sasa[i]\n",
        "        max_sasa = MAX_SASA.get(res_name, 0.0)\n",
        "        if max_sasa == 0.0:\n",
        "            print(f\"Warning: No max SASA value for residue {res_name}. Skipping.\")\n",
        "            continue\n",
        "        rsa = residue_sasa / max_sasa if max_sasa > 0 else 0.0\n",
        "        rsa = min(rsa, 1.0)\n",
        "        rsa_values.append((res_name, res_id, rsa))\n",
        "    # return rsa_values\n",
        "    return torch.tensor([rsa for _, _, rsa in rsa_values], dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "\n",
        "def calculate_depth(structure, sequence_length):\n",
        "    depth_per_residue = {}\n",
        "    atoms = Selection.unfold_entities(structure, \"A\")  # Get all atoms\n",
        "    for residue in Selection.unfold_entities(structure, \"R\"):\n",
        "        res_id = (residue.get_parent().id, residue.id[1])\n",
        "        min_dist = float(\"inf\")\n",
        "        for atom in residue:\n",
        "            for other_atom in atoms:\n",
        "                if other_atom.get_parent() != residue:\n",
        "                    dist = np.linalg.norm(atom.coord - other_atom.coord)\n",
        "                    min_dist = min(min_dist, dist)\n",
        "        depth_per_residue[res_id] = min_dist\n",
        "\n",
        "    # Assign default value for missing residues\n",
        "    depth_values = [0.0] * sequence_length  # Or use average depth if available\n",
        "    for i, residue in enumerate(structure.get_residues()):\n",
        "        res_id = (residue.get_parent().id, residue.id[1])\n",
        "        if res_id in depth_per_residue:\n",
        "            depth_values[i] = depth_per_residue[res_id]\n",
        "\n",
        "    return torch.tensor(depth_values, dtype=torch.float32).unsqueeze(1)\n",
        "    # return depth_values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONW3_zwXJa24"
      },
      "outputs": [],
      "source": [
        "def fuse_features(esm2_embeddings, ss_onehot, phi_angles,\n",
        "                  psi_angles, b_factors):\n",
        "    \"\"\"\n",
        "    Fuse ESM-2 embeddings with structural features.\n",
        "\n",
        "    Args:\n",
        "        esm2_embeddings (torch.Tensor): Shape [num_residues, 1280]\n",
        "        ss_onehot (torch.Tensor): Shape [num_residues, 4]\n",
        "        phi_angles (torch.Tensor): Shape [num_residues, 1]\n",
        "        psi_angles (torch.Tensor): Shape [num_residues, 1]\n",
        "        b_factors (torch.Tensor): Shape [num_residues, 1]\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Fused node features, shape [num_residues, 1287] (1280 + 4 + 1 + 1 + 1)\n",
        "    \"\"\"\n",
        "    # Ensure all features have the same length\n",
        "    num_residues = esm2_embeddings.shape[0]\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    print(f\"esm2_embeddings shape: {esm2_embeddings.shape}\")\n",
        "    print(f\"ss_onehot shape: {ss_onehot.shape}\")\n",
        "    print(f\"phi_angles shape: {phi_angles.shape}\")\n",
        "    print(f\"psi_angles shape: {psi_angles.shape}\")\n",
        "    print(f\"b_factors shape: {b_factors.shape}\")\n",
        "    # print(f\"residue_depths shape: {residue_depths.shape}\")\n",
        "\n",
        "    # Adjust ss_onehot, phi_angles, psi_angles, b_factors if needed\n",
        "    min_length = min(num_residues, ss_onehot.shape[0], phi_angles.shape[0],\n",
        "                     psi_angles.shape[0], b_factors.shape[0])\n",
        "\n",
        "    esm2_embeddings = esm2_embeddings[:min_length]\n",
        "    ss_onehot = ss_onehot[:min_length]\n",
        "    phi_angles = phi_angles[:min_length]\n",
        "    psi_angles = psi_angles[:min_length]\n",
        "    b_factors = b_factors[:min_length]\n",
        "\n",
        "    # Concatenate all features\n",
        "    node_features = torch.cat([\n",
        "        esm2_embeddings,  # [num_residues, 1280]\n",
        "        ss_onehot,        # [num_residues, 4]\n",
        "        phi_angles,       # [num_residues, 1]\n",
        "        psi_angles,       # [num_residues, 1]\n",
        "        b_factors        # [num_residues, 1]\n",
        "    ], dim=1)  # Shape: [num_residues, 1287]\n",
        "\n",
        "    return node_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9Aq22_TlYW_"
      },
      "outputs": [],
      "source": [
        "def create_edge_features(distances, threshold=8.0):\n",
        "    num_residues = distances.shape[0]\n",
        "    contact_map = (distances < threshold) & (distances > 0)\n",
        "\n",
        "    # Restrict indices to be within the valid node range (0 to num_residues - 1)\n",
        "    edge_index = torch.nonzero(torch.tensor(contact_map, dtype=torch.bool), as_tuple=False).t()\n",
        "    edge_index = edge_index[:, :num_residues]  # Ensure indices are within bounds\n",
        "\n",
        "    # Vectorized edge attribute computation\n",
        "    src, dst = edge_index[0], edge_index[1]\n",
        "    dists = torch.tensor(distances[src, dst], dtype=torch.float32)\n",
        "    seq_seps = torch.abs(src - dst).float()\n",
        "\n",
        "    # Normalize features\n",
        "    dists = dists / threshold  # Scale to 0–1\n",
        "    seq_seps = seq_seps / num_residues  # Scale to 0–1\n",
        "\n",
        "    edge_attr = torch.stack([dists, seq_seps], dim=1)  # Shape: [num_edges, 2]\n",
        "\n",
        "    return edge_index, edge_attr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVUIgi7WA4g3"
      },
      "outputs": [],
      "source": [
        "def create_graph_data(node_features, edge_index, edge_attr, labels):\n",
        "    \"\"\"\n",
        "    Create a PyTorch Geometric Data object for the protein graph.\n",
        "\n",
        "    Args:\n",
        "        node_features (torch.Tensor): Shape [num_residues, 1288], fused node features.\n",
        "        edge_index (torch.Tensor): Shape [2, num_edges], indices of connected nodes.\n",
        "        edge_attr (torch.Tensor): Shape [num_edges, 2], edge features.\n",
        "        labels (torch.Tensor): Shape [num_residues], binary labels (0 or 1) for binding sites.\n",
        "\n",
        "    Returns:\n",
        "        Data: PyTorch Geometric Data object.\n",
        "    \"\"\"\n",
        "    return Data(\n",
        "        x=node_features,      # Node features\n",
        "        edge_index=edge_index,  # Edge indices\n",
        "        edge_attr=edge_attr,  # Edge features\n",
        "        y=labels              # Labels for binding site prediction\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne4lR4G5E_PG"
      },
      "source": [
        "### Test feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNmODd-p451i"
      },
      "outputs": [],
      "source": [
        "sample_idx = 540\n",
        "sample_prot_id = train_df.iloc[sample_idx]['prot_id']\n",
        "sample_sequence = train_df.iloc[sample_idx]['sequence']\n",
        "sample_sequence_len = len(train_df.iloc[sample_idx]['sequence'])\n",
        "sample_labels = train_df.iloc[sample_idx]['any_ligand_binding_sites']\n",
        "sample_train_embeddings = train_embeddings[sample_idx]\n",
        "\n",
        "sample_structure_file = f\"/content/drive/MyDrive/Protein-binding/esmFold_pdb_files/{sample_prot_id}.pdb\"\n",
        "\n",
        "sample_structure = get_structure(sample_prot_id, sample_structure_file)\n",
        "sample_coordinates = extract_coordinates(sample_structure)\n",
        "sample_distances = calculate_residue_distances(sample_coordinates)\n",
        "sample_ss_one_hot = get_secondary_structure_mdtraj(sample_structure_file, sample_sequence_len)['one_hot'].to(device)\n",
        "sample_phi_angles, sample_psi_angles = get_dihedral_angles(sample_structure_file, sample_sequence_len)\n",
        "sample_b_factors = get_b_factors(sample_structure, sample_sequence_len).to(device)\n",
        "sample_rsa_values = compute_rsa(sample_structure_file)\n",
        "min_length = 1\n",
        "\n",
        "# print(sample_rsa_values)\n",
        "# rsa_dict = {res_id: rsa for _, res_id, rsa in sample_rsa_values}\n",
        "# sample_rsa_tensor = torch.zeros((min_length, 1), dtype=torch.float)\n",
        "# print(sample_rsa_tensor)\n",
        "\n",
        "# for i, res_id in enumerate(residue_ids[:min_length]):\n",
        "#     sample_rsa_tensor[i, 0] = rsa_dict.get(res_id, 0.0)\n",
        "\n",
        "# print(sample_prot_id)\n",
        "# print(sample_sequence)\n",
        "# print(len(sample_sequence))\n",
        "# print(sample_b_factors.shape)\n",
        "# print(sample_ss_one_hot.shape)\n",
        "# print(sample_phi_angles.shape)\n",
        "# print(sample_psi_angles.shape)\n",
        "# print(sample_rsa_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYNgWL539Lki"
      },
      "outputs": [],
      "source": [
        "# edge_index, edge_attr = create_edge_features(sample_distances)\n",
        "# node_features = fuse_features(sample_train_embeddings.to(device), sample_ss_one_hot,\n",
        "#                               sample_phi_angles.to(device), sample_psi_angles.to(device), sample_b_factors)\n",
        "# labels = torch.tensor(sample_labels, dtype = torch.long).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YODKFrhsAFee"
      },
      "outputs": [],
      "source": [
        "# graph_data = create_graph_data(node_features, edge_index, edge_attr, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgSyv-CAtfiy"
      },
      "outputs": [],
      "source": [
        "# print(len(train_df))\n",
        "# print(len(train_labels))\n",
        "# print(len(train_seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT4TvASC9aOi"
      },
      "source": [
        "### Prepare data tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8zjZG1s9dL4"
      },
      "outputs": [],
      "source": [
        "def get_graph_data(df, embeddings, device):\n",
        "    graph_data_list = []\n",
        "\n",
        "    for idx in tqdm(range(len(df))):\n",
        "        prot_id = df.iloc[idx]['prot_id']\n",
        "        sequence = df.iloc[idx]['sequence']\n",
        "        sequence_len = len(df.iloc[idx]['sequence'])\n",
        "        labels = df.iloc[idx]['any_ligand_binding_sites']\n",
        "        embedding = torch.tensor(embeddings[idx]['embeddings'])\n",
        "\n",
        "        try:\n",
        "            structure_file = f\"/content/drive/MyDrive/Protein-binding/esmFold_pdb_files/{prot_id}.pdb\"\n",
        "            structure = get_structure(prot_id, structure_file)\n",
        "            coordinates = extract_coordinates(structure)\n",
        "            distances = calculate_residue_distances(coordinates)\n",
        "            # depths = calculate_depth(structure, sequence_len)\n",
        "            # rsa_values = compute_rsa(structure_file)\n",
        "            ss_one_hot = get_secondary_structure_mdtraj(structure_file, sequence_len)['one_hot'].to(device)\n",
        "            phi_angles, psi_angles = get_dihedral_angles(structure_file, sequence_len)\n",
        "            b_factors = get_b_factors(structure, sequence_len).to(device)\n",
        "        except Exception as e:\n",
        "            # print(f\"PDB file of {prot_id} ID cannot be found\")\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "        edge_index, edge_attr = create_edge_features(distances)\n",
        "        node_features = fuse_features(embedding.to(device), ss_one_hot,\n",
        "                                      phi_angles.to(device), psi_angles.to(device),\n",
        "                                      b_factors)\n",
        "        labels = torch.tensor(labels, dtype = torch.long).to(device)\n",
        "        print(f\"Shape of labels: {labels.shape}\")\n",
        "        graph_data = create_graph_data(node_features, edge_index, edge_attr, labels)\n",
        "\n",
        "        graph_data_list.append(graph_data)\n",
        "\n",
        "    return graph_data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXHJmq2gBOEf",
        "outputId": "871f6d33-8ea7-4013-aa9a-d308a7dd8dda"
      },
      "outputs": [],
      "source": [
        "train_graphs_data = get_graph_data(train_df, train_embeddings, device)\n",
        "# test_graphs_data = get_graph_data(test_df, test_embeddings, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE7D3CthzLJP"
      },
      "source": [
        "### Create baseline GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJrQZtDyFoxI"
      },
      "outputs": [],
      "source": [
        "class BindingSiteGCN(nn.Module):\n",
        "    def __init__(self, node_dim, edge_dim=2, hidden_dim=512):\n",
        "        \"\"\"\n",
        "        A baseline GCN for binding site prediction.\n",
        "\n",
        "        Args:\n",
        "            node_dim (int): Dimension of node features (1288 in your case).\n",
        "            edge_dim (int): Dimension of edge features (2 in your case).\n",
        "            hidden_dim (int): Hidden dimension for GCN layers.\n",
        "        \"\"\"\n",
        "        super(BindingSiteGCN, self).__init__()\n",
        "        # GCN layers with edge features\n",
        "        self.conv1 = GCNConv(node_dim, hidden_dim)  # Remove edge_dim\n",
        "        self.conv2 = GCNConv(hidden_dim, 256)  # Remove edge_dim\n",
        "        self.conv3 = GCNConv(256, 128)  # Remove edge_dim\n",
        "\n",
        "        # Linear layer to process edge features\n",
        "        self.edge_lin = nn.Linear(edge_dim, hidden_dim)\n",
        "\n",
        "        # Final classifier\n",
        "        # self.fc = nn.Linear(128, 2)  # 2 classes: 0 (non-binding), 1 (binding)\n",
        "        self.pre_fc = nn.Linear(128, 16)  # Reduce dimension before KAN\n",
        "        self.fc = KAN([16, 8, 2], grid=2, k=2)\n",
        "\n",
        "        # Activation and dropout\n",
        "        self.activation_func = nn.LeakyReLU(negative_slope=0.1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        # Get the number of nodes in the current batch\n",
        "\n",
        "        # Process edge features separately\n",
        "        edge_attr = self.edge_lin(edge_attr)\n",
        "\n",
        "        # GCN layers\n",
        "        # Pass num_nodes to gcn_norm\n",
        "        x = self.conv1(x, edge_index)  # Removed edge_attr here, Added num_nodes\n",
        "        x = self.activation_func(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index) # Removed edge_attr here, Added num_nodes\n",
        "        x = self.activation_func(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv3(x, edge_index) # Removed edge_attr here, Added num_nodes\n",
        "        x = self.activation_func(x)\n",
        "\n",
        "        # Final classification\n",
        "        x = self.fc(x)  # Shape: [num_residues, 2]\n",
        "\n",
        "        return x  # Logits for each residue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcNLiSgK_T5e"
      },
      "outputs": [],
      "source": [
        "class BindingSiteGAT(nn.Module):\n",
        "    def __init__(self, node_dim=1287, hidden_dim=512, heads=4):\n",
        "        \"\"\"\n",
        "        A GAT-based model for binding site prediction with a KAN classifier.\n",
        "\n",
        "        Args:\n",
        "            node_dim (int): Dimension of node features (e.g., 1287 for ESM-2 + 7 features).\n",
        "            hidden_dim (int): Hidden dimension for GAT layers.\n",
        "            heads (int): Number of attention heads in GAT layers.\n",
        "        \"\"\"\n",
        "        super(BindingSiteGAT, self).__init__()\n",
        "        # GAT layers with multi-head attention\n",
        "        self.conv1 = GATConv(node_dim, hidden_dim, heads=heads, concat=True)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, 256, heads=heads, concat=True)\n",
        "        self.conv3 = GATConv(256 * heads, 128, heads=1, concat=False)  # Single head for final layer\n",
        "\n",
        "        # KAN layer for classification\n",
        "        self.pre_fc = nn.Linear(128, 16)  # Reduce dimension before KAN\n",
        "        self.fc = KAN([16, 8, 2], grid=2, k=2)  # Smaller KAN with reduced spline complexity\n",
        "\n",
        "        # Activation and dropout\n",
        "        self.activation_func = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index  # edge_attr is unused in GATConv\n",
        "\n",
        "        # GAT layers\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.activation_func(x)\n",
        "\n",
        "        # KAN classifier\n",
        "        x = self.fc(x)  # Shape: [num_residues, 2]\n",
        "\n",
        "        return x  # Logits for each residue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHiWoOS126CQ",
        "outputId": "6b841afd-da8c-432c-e12b-aea80aed2258"
      },
      "outputs": [],
      "source": [
        "train_graphs, val_graphs = train_test_split(\n",
        "    train_graphs_data,\n",
        "    test_size=0.05,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Number of training graphs: {len(train_graphs)}\")\n",
        "print(f\"Number of validation graphs: {len(val_graphs)}\")\n",
        "# print(f\"Number of testing graphs: {len(test_graphs_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8s9CIW3LOPH"
      },
      "outputs": [],
      "source": [
        "for idx, train_graph in enumerate(train_graphs):\n",
        "    if train_graph.x.shape[0] != train_graph.y.shape[0]:\n",
        "        print(f\"Abnormal at index: {idx}\")\n",
        "        print(f\"Train graph's input shape: {train_graph['x'].shape[0]} and labels shape: {train_graph['y'].shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_fbEvzd8XqG"
      },
      "outputs": [],
      "source": [
        "for idx, val_graph in enumerate(val_graphs):\n",
        "    if val_graph.x.shape[0] != val_graph.y.shape[0]:\n",
        "        print(f\"Abnormal at index: {idx}\")\n",
        "        print(f\"Train graph's input shape: {val_graph['x'].shape[0]} and labels shape: {val_graph['y'].shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2iJGH6dLHmz"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXjElh_VK8fs"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
        "eval_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)\n",
        "# test_loader = DataLoader(test_graphs_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seUK4wibD_zC"
      },
      "source": [
        "### Customized loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyTufMGUEC-8"
      },
      "outputs": [],
      "source": [
        "class WeightedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, pos_weight):\n",
        "        super().__init__()\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        # Handle class imbalance with weighted loss\n",
        "        weight = torch.tensor([1.0, self.pos_weight]).to(logits.device)  # [weight for class 0, weight for class 1]\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=weight)\n",
        "        return loss_fct(logits, labels)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # Weight for the positive class\n",
        "        self.gamma = gamma  # Focusing parameter\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        # Compute the cross-entropy loss (without reduction)\n",
        "        ce_loss = nn.functional.cross_entropy(logits, labels, reduction='none')\n",
        "\n",
        "        # Compute the probability of the true class\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        true_probs = probs[torch.arange(probs.size(0), device=probs.device), labels]\n",
        "\n",
        "        # Compute the focal loss term: (1 - p_t)^gamma\n",
        "        focal_term = (1 - true_probs) ** self.gamma\n",
        "\n",
        "        # Apply the alpha weighting\n",
        "        alpha_weight = torch.where(labels == 1, self.alpha, 1.0 - self.alpha).to(logits.device)\n",
        "\n",
        "        # Compute the focal loss\n",
        "        loss = alpha_weight * focal_term * ce_loss\n",
        "\n",
        "        # Apply reduction\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "class PositionAwareLoss(nn.Module):\n",
        "    def __init__(self, pos_weight, position_weight, alpha=0.25, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.weighted_ce = WeightedCrossEntropyLoss(pos_weight)\n",
        "        self.focal_loss = FocalLoss(alpha=alpha, gamma=gamma, reduction='mean')\n",
        "        self.position_weight = position_weight\n",
        "\n",
        "    def forward(self, logits, labels, batch=None):\n",
        "        # Compute the base loss (weighted cross-entropy + focal loss)\n",
        "        ce_loss = self.weighted_ce(logits, labels)\n",
        "        focal_loss = self.focal_loss(logits, labels)\n",
        "        base_loss = ce_loss + focal_loss\n",
        "\n",
        "        # Position-aware component\n",
        "        probs = torch.softmax(logits, dim=-1)[:, 1]  # Get binding probabilities [total_num_nodes]\n",
        "        position_loss = torch.tensor(0.0).to(logits.device)\n",
        "\n",
        "        # Since we're using a GNN, we need to account for the graph structure\n",
        "        # batch.batch indicates which nodes belong to which graph\n",
        "        if batch is not None and batch.num_graphs == 1:  # Single graph per batch\n",
        "            num_nodes = batch.num_nodes\n",
        "            # Penalize offset predictions by checking neighboring nodes\n",
        "            for i in range(1, num_nodes - 1):\n",
        "                # Encourage predictions to match true binding site positions\n",
        "                if labels[i] == 1 or labels[i-1] == 1 or labels[i+1] == 1:\n",
        "                    position_loss += torch.abs(probs[i] - (labels[i] == 1).float())\n",
        "\n",
        "        return base_loss + self.position_weight * position_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RFBr67RLR3T",
        "outputId": "388408af-d2d7-48e5-f327-b64ba62c82fe"
      },
      "outputs": [],
      "source": [
        "input_train_embedding_dim = torch.tensor(train_embeddings[0]['embeddings']).shape[1] + 7\n",
        "\n",
        "model = BindingSiteGCN(node_dim = input_train_embedding_dim, edge_dim=2, hidden_dim=512).to(device)\n",
        "# model = BindingSiteGAT(node_dim = input_train_embedding_dim, hidden_dim=512, heads=4).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# class_weights = torch.tensor([0.2, 0.8]).to(device)\n",
        "pos_weight = 5.0\n",
        "position_weight = 1\n",
        "focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n",
        "# criterion = WeightedCrossEntropyLoss(pos_weight=pos_weight)\n",
        "criterion = PositionAwareLoss(pos_weight=pos_weight, position_weight=position_weight, alpha=0.25, gamma=2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2XiIrm4LphW"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch)\n",
        "\n",
        "            # Debug: Print shapes\n",
        "            # print(f\"Batch num_graphs: {batch.num_graphs}\")\n",
        "            # print(f\"Batch num_nodes: {batch.num_nodes}\")\n",
        "            # print(f\"Out shape: {out.shape}\")\n",
        "            # print(f\"Labels shape: {batch.y.shape}\")\n",
        "\n",
        "            probs = torch.softmax(out, dim=1)[:, 1]\n",
        "            preds = torch.argmax(out, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"auc\": auc,\n",
        "        \"mcc\": mcc\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv02B07VpFTZ",
        "outputId": "db548b10-5fe2-4ee6-c08a-f9f3230a94cf"
      },
      "outputs": [],
      "source": [
        "gc.collect()  # Force garbage collection to potentially free up memory\n",
        "# torch.cuda.empty_cache()  # Empty the CUDA cache\n",
        "# model = model.half()  # Convert model parameters to half-precision\n",
        "# torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rMjFPUULibO",
        "outputId": "557fbdf6-261b-4f94-c9be-b2d8b68192ff"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "best_val_f1 = 0\n",
        "best_model_state = None\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # print(f\"Batch shape: {batch}\")\n",
        "        out = model(batch)\n",
        "        # print(f\"Out shape: {out.shape}\")\n",
        "        # print(f\"Labels shape: {batch.y.shape}\")\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    val_metrics = evaluate(model, eval_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Validation Precision: {val_metrics['precision']:.4f}\")\n",
        "    print(f\"Validation Recall: {val_metrics['recall']:.4f}\")\n",
        "    print(f\"Validation F1-Score: {val_metrics['f1']:.4f}\")\n",
        "    print(f\"Validation AUC-ROC: {val_metrics['auc']:.4f}\")\n",
        "\n",
        "    if val_metrics['f1'] > best_val_f1:\n",
        "        best_val_f1 = val_metrics['f1']\n",
        "        best_model_state = model.state_dict()\n",
        "        print(\"Best validation F1-score improved! Saving model state.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsUq8AXLwKKn"
      },
      "outputs": [],
      "source": [
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "with open('/content/drive/MyDrive/Protein-binding/data/test300_loader.pkl', 'rb') as f:\n",
        "    saved_test_loader = pickle.load(f)\n",
        "\n",
        "model.eval()\n",
        "test_metrics = evaluate(model, saved_test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28QWHMiIB5dy",
        "outputId": "6cbba220-053b-4675-9ef9-02055f850510"
      },
      "outputs": [],
      "source": [
        "print(test_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_dyEUfdplyU",
        "outputId": "a496bf1b-1707-421e-cb74-2f7f206a4798"
      },
      "outputs": [],
      "source": [
        "print(saved_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O2SkEDeh5Wp"
      },
      "outputs": [],
      "source": [
        "torch.save(best_model_state, saved_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzwgZA5Kaeeg"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/Protein-binding/data/test300_loader.pkl', 'wb') as f:\n",
        "#     pickle.dump(test_loader, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0eToZ8tZ5pp"
      },
      "source": [
        "### Save and load model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFikQDmp0fdE"
      },
      "outputs": [],
      "source": [
        "def create_labels(data_loader):\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = batch.to(device)\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "    return all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGafSRf5aVVG",
        "outputId": "f20b972b-911a-410a-9191-98ff66ca03f5"
      },
      "outputs": [],
      "source": [
        "def inference(model_name, data_loader):\n",
        "    all_preds = []\n",
        "    model_instance = BindingSiteGCN(node_dim=input_train_embedding_dim, edge_dim=2, hidden_dim=512).to(device)\n",
        "    model_instance.load_state_dict(torch.load(model_name))\n",
        "    model_instance.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model_instance(batch)\n",
        "\n",
        "            probs = torch.softmax(out, dim=1)[:, 1]\n",
        "            preds = torch.argmax(out, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    return all_preds\n",
        "\n",
        "nuclear_model_name = \"/content/drive/MyDrive/Protein-binding/trained_models/GCN_KAN_for_nuclear_binding_20Apr_model.pth\"\n",
        "nuclear_preds = inference(nuclear_model_name, saved_test_loader)\n",
        "\n",
        "metal_model_name = \"/content/drive/MyDrive/Protein-binding/trained_models/GCN_KAN_for_metal_binding_20Apr_model.pth\"\n",
        "metal_preds = inference(metal_model_name, saved_test_loader)\n",
        "\n",
        "small_model_name = \"/content/drive/MyDrive/Protein-binding/trained_models/GCN_KAN_for_small_binding_20Apr_model.pth\"\n",
        "small_preds = inference(small_model_name, saved_test_loader)\n",
        "\n",
        "merged_preds_list = [max(a, b, c) for a, b, c in zip(nuclear_preds, metal_preds, small_preds)]\n",
        "print(merged_preds_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAdiMcQS7F0v",
        "outputId": "d48b2ea7-2273-4e91-b5e5-86e54926bba5"
      },
      "outputs": [],
      "source": [
        "# print(len(merged_preds_list))\n",
        "all_labels = create_labels(saved_test_loader)\n",
        "print(len(all_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE3UNqTl1q4P",
        "outputId": "6d6ab1b5-d6c3-4589-da04-cb08f5ad93d3"
      },
      "outputs": [],
      "source": [
        "metal_precision, metal_recall, metal_f1, _ = precision_recall_fscore_support(all_labels, metal_preds, average='binary')\n",
        "metal_mcc = matthews_corrcoef(all_labels, metal_preds)\n",
        "\n",
        "nuclear_precision, nuclear_recall, nuclear_f1, _ = precision_recall_fscore_support(all_labels, nuclear_preds, average='binary')\n",
        "nuclear_mcc = matthews_corrcoef(all_labels, nuclear_preds)\n",
        "\n",
        "small_precision, small_recall, small_f1, _ = precision_recall_fscore_support(all_labels, small_preds, average='binary')\n",
        "small_mcc = matthews_corrcoef(all_labels, small_preds)\n",
        "\n",
        "overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(all_labels, merged_preds_list, average='binary')\n",
        "overall_mcc = matthews_corrcoef(all_labels, merged_preds_list)\n",
        "\n",
        "\n",
        "print(f\"Metal Precision: {metal_precision:.4f}\")\n",
        "print(f\"Metal Recall: {metal_recall:.4f}\")\n",
        "print(f\"Metal F1-Score: {metal_f1:.4f}\")\n",
        "print(f\"Metal MCC score: {metal_mcc:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"Nuclear Precision: {nuclear_precision:.4f}\")\n",
        "print(f\"Nuclear Recall: {nuclear_recall:.4f}\")\n",
        "print(f\"Nuclear F1-Score: {nuclear_f1:.4f}\")\n",
        "print(f\"Nuclear MCC score: {nuclear_mcc:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"Small Precision: {small_precision:.4f}\")\n",
        "print(f\"Small Recall: {small_recall:.4f}\")\n",
        "print(f\"Small F1-Score: {small_f1:.4f}\")\n",
        "print(f\"Small MCC score: {small_mcc:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"Overall Precision: {overall_precision:.4f}\")\n",
        "print(f\"Overall Recoverall: {overall_recall:.4f}\")\n",
        "print(f\"Overall F1-Score: {overall_f1:.4f}\")\n",
        "print(f\"Overall MCC score: {overall_mcc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSi3_qclIqqD"
      },
      "source": [
        "### Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4k_hcmbB7Ed"
      },
      "outputs": [],
      "source": [
        "# all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for test_batch in test_loader:\n",
        "#         test_batch = test_batch.to(device)\n",
        "#         out = model(test_batch)\n",
        "\n",
        "#         probs = torch.softmax(out, dim=1)[:, 1]\n",
        "#         preds = torch.argmax(out, dim=1)\n",
        "\n",
        "#         all_preds.extend(preds.cpu().numpy())\n",
        "#         all_labels.extend(batch.y.cpu().numpy())\n",
        "#         all_probs.extend(probs.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7miY1ZXSGNft"
      },
      "outputs": [],
      "source": [
        "# all_test_prot_sequences = test_df['sequence'].tolist()\n",
        "# all_sequences = []\n",
        "# for prot_seq in all_test_prot_sequences:\n",
        "#     all_sequences.extend(list(prot_seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NP1i-x1GZgH"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "# from copy import deepcopy\n",
        "\n",
        "# test_aa_counter = Counter(all_sequences)\n",
        "# false_negatives_dict = {}\n",
        "\n",
        "# for prob, label, pred, acid in zip(all_probs, all_labels, all_preds, all_sequences):\n",
        "#     if label == 1 and pred == 0:\n",
        "#         print(f\"Acid: {acid} and its probability: {prob}\")\n",
        "\n",
        "#         if acid not in false_negatives_dict:\n",
        "#             false_negatives_dict[acid] = 1\n",
        "#         else:\n",
        "#             false_negatives_dict[acid] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93yKA4nPHTw4"
      },
      "outputs": [],
      "source": [
        "# false_negatives_percentage_dict = deepcopy(false_negatives_dict)\n",
        "\n",
        "# for acid in false_negatives_percentage_dict:\n",
        "#     false_negatives_percentage_dict[acid] /= test_aa_counter[acid]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQbxqDenMI6O"
      },
      "outputs": [],
      "source": [
        "# # print(false_negatives_percentage_dict)\n",
        "# sorted_false_negatives_percentage_dict = dict(sorted(false_negatives_percentage_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "# print(sorted_false_negatives_percentage_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZF5EOehNgpS"
      },
      "outputs": [],
      "source": [
        "# sorted_false_negatives_percentage_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1vkoetKc0rq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
