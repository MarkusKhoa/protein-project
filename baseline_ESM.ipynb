{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Bio import SeqIO\n",
    "from glob import glob\n",
    "from transformers import (AutoModelForTokenClassification, AutoTokenizer, \n",
    "                          AutoModelForMaskedLM,\n",
    "                           EsmForMaskedLM, EsmTokenizer,\n",
    "                           TrainingArguments\n",
    "                        )\n",
    "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
    "                             matthews_corrcoef, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import convert_to_binary_list\n",
    "from pprint import pprint\n",
    "from datasets import Dataset\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/development_set/all_binding_sites_batch_10.csv', 'data/development_set/all_binding_sites_batch_4.csv', 'data/development_set/all_binding_sites_batch_8.csv', 'data/development_set/all_binding_sites_batch_7.csv', 'data/development_set/all_binding_sites_batch_5.csv', 'data/development_set/all_binding_sites_batch_3.csv', 'data/development_set/all_binding_sites_batch_11.csv', 'data/development_set/all_binding_sites_batch_1.csv', 'data/development_set/all_binding_sites_batch_2.csv', 'data/development_set/all_binding_sites_batch_6.csv']\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"data/development_set/\"\n",
    "csv_files = glob(dir_path + \"/*.csv\")\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prot_id</th>\n",
       "      <th>binding_sites</th>\n",
       "      <th>ligand_type</th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>binary_binding_sites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P59857</td>\n",
       "      <td>[60, 65, 71, 76, 77, 80, 81, 83, 84, 87, 88, 9...</td>\n",
       "      <td>small</td>\n",
       "      <td>MQDAITSVINAADVQGKYLDDSSVEKLRGYFQTGELRVRAAATIAA...</td>\n",
       "      <td>161</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P26514</td>\n",
       "      <td>[384, 388, 408, 409, 410, 417, 419, 421, 424, ...</td>\n",
       "      <td>small</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P12111</td>\n",
       "      <td>[3128, 3153, 3154, 3155]</td>\n",
       "      <td>small</td>\n",
       "      <td>MRKHRHLPLVAVFCLFLSGFPTTHAQQQQADVKNGAAADIIFLVDS...</td>\n",
       "      <td>3177</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P05057</td>\n",
       "      <td>[141, 142, 145, 149, 37, 38, 39, 42, 49, 50, 5...</td>\n",
       "      <td>small</td>\n",
       "      <td>MNGPIIMTREERMKIVHEIKERILDKYGDDVKAIGVYGSLGRQTDG...</td>\n",
       "      <td>253</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P46859</td>\n",
       "      <td>[160, 159, 163, 40, 17, 18, 19, 20, 21, 22, 23...</td>\n",
       "      <td>small</td>\n",
       "      <td>MSTTNHDHHIYVLMGVSGSGKSAVASEVAHQLHAAFLDGDFLHPRR...</td>\n",
       "      <td>175</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Q14114</td>\n",
       "      <td>[64, 67, 69, 71, 103, 106, 108, 77, 110, 78, 1...</td>\n",
       "      <td>metal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>O32553</td>\n",
       "      <td>[32, 205, 143, 145, 219, 221]</td>\n",
       "      <td>metal</td>\n",
       "      <td>MSKDIKQVIEIAKKHNLFLKEETIQFNESGLDFQAVFAQDNNGIDW...</td>\n",
       "      <td>302</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>A8IYS5</td>\n",
       "      <td>[146, 124]</td>\n",
       "      <td>metal</td>\n",
       "      <td>MSSTYQKFAASLREQEGPSGSLPTNGPSTTTPFANATNRYLNNHSG...</td>\n",
       "      <td>481</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>H2EMX8</td>\n",
       "      <td>[80, 113, 110]</td>\n",
       "      <td>metal</td>\n",
       "      <td>MEEAKVEAKDGTISVATAFAGHQQAVLDSDHKFLTQAVEEAYKGVD...</td>\n",
       "      <td>185</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Q54XS1</td>\n",
       "      <td>[318, 273, 278]</td>\n",
       "      <td>metal</td>\n",
       "      <td>MESNTNSQGQGIIPQSYHSSIFFSISKGSDKIGGLLEYLEIIKKHN...</td>\n",
       "      <td>441</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    prot_id                                      binding_sites ligand_type  \\\n",
       "0    P59857  [60, 65, 71, 76, 77, 80, 81, 83, 84, 87, 88, 9...       small   \n",
       "1    P26514  [384, 388, 408, 409, 410, 417, 419, 421, 424, ...       small   \n",
       "2    P12111                           [3128, 3153, 3154, 3155]       small   \n",
       "3    P05057  [141, 142, 145, 149, 37, 38, 39, 42, 49, 50, 5...       small   \n",
       "4    P46859  [160, 159, 163, 40, 17, 18, 19, 20, 21, 22, 23...       small   \n",
       "..      ...                                                ...         ...   \n",
       "995  Q14114  [64, 67, 69, 71, 103, 106, 108, 77, 110, 78, 1...       metal   \n",
       "996  O32553                      [32, 205, 143, 145, 219, 221]       metal   \n",
       "997  A8IYS5                                         [146, 124]       metal   \n",
       "998  H2EMX8                                     [80, 113, 110]       metal   \n",
       "999  Q54XS1                                    [318, 273, 278]       metal   \n",
       "\n",
       "                                              sequence  sequence_length  \\\n",
       "0    MQDAITSVINAADVQGKYLDDSSVEKLRGYFQTGELRVRAAATIAA...              161   \n",
       "1                                                  NaN                0   \n",
       "2    MRKHRHLPLVAVFCLFLSGFPTTHAQQQQADVKNGAAADIIFLVDS...             3177   \n",
       "3    MNGPIIMTREERMKIVHEIKERILDKYGDDVKAIGVYGSLGRQTDG...              253   \n",
       "4    MSTTNHDHHIYVLMGVSGSGKSAVASEVAHQLHAAFLDGDFLHPRR...              175   \n",
       "..                                                 ...              ...   \n",
       "995                                                NaN                0   \n",
       "996  MSKDIKQVIEIAKKHNLFLKEETIQFNESGLDFQAVFAQDNNGIDW...              302   \n",
       "997  MSSTYQKFAASLREQEGPSGSLPTNGPSTTTPFANATNRYLNNHSG...              481   \n",
       "998  MEEAKVEAKDGTISVATAFAGHQQAVLDSDHKFLTQAVEEAYKGVD...              185   \n",
       "999  MESNTNSQGQGIIPQSYHSSIFFSISKGSDKIGGLLEYLEIIKKHN...              441   \n",
       "\n",
       "                                  binary_binding_sites  \n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1                                                   []  \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "..                                                 ...  \n",
       "995                                                 []  \n",
       "996  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "998  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "999  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "binding_sites_df = pd.DataFrame()\n",
    "\n",
    "for f in csv_files:\n",
    "    batch_df = pd.read_csv(f)\n",
    "    binding_sites_df = pd.concat([binding_sites_df, batch_df])\n",
    "\n",
    "# New concated binding sites df\n",
    "display(binding_sites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97754/2411430571.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  needed_binding_sites_df['binary_binding_sites'] = [\n"
     ]
    }
   ],
   "source": [
    "needed_binding_sites_df = binding_sites_df[binding_sites_df['sequence_length'] > 0]\n",
    "needed_binding_sites_df['binary_binding_sites'] = [\n",
    "            convert_to_binary_list(row['binding_sites'], row['sequence_length']) \n",
    "            for _, row in needed_binding_sites_df.iterrows()\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ligand_type\n",
       "metal      4163\n",
       "small      1291\n",
       "nuclear     440\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needed_binding_sites_df.ligand_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "MAQTPAFNKPKVELHVHLDGAIKPETILYYGRKRGIALPADTPEELQNIIGMDKPLSLPEFLAKFDYYMPAIAGCREAVKRIAYEFVEMKAKDGVVYVEVRYSPHLLANSKVEPIPWNQAEGDLTPDEVVSLVNQGLQEGERDFGVKVRSILCCMRHQPSWSSEVVELCKKYREQTVVAIDLAGDETIEGSSLFPGHVKAYAEAVKSGVHRTVHAGEVGSANVVKEAVDTLKTERLGHGYHTLEDATLYNRLRQENMHFEVCPWSSYLTGAWKPDTEHPVVRFKNDQVNYSLNTDDPLIFKSTLDTDYQMTKNEMGFTEEEFKRLNINAAKSSFLPEDEKKELLDLLYKAYGMPSPASAEQCL\n",
      "363\n",
      "363\n",
      "[269, 15, 17, 19, 153, 155, 157, 295, 296, 52, 183, 56, 185, 58, 57, 184, 61, 62, 65, 214, 217, 218, 102, 106, 238, 240, 117]\n"
     ]
    }
   ],
   "source": [
    "# Data verification\n",
    "\n",
    "sample_idx = 10\n",
    "seq_label = needed_binding_sites_df.iloc[sample_idx]['binary_binding_sites']\n",
    "seq_len = needed_binding_sites_df.iloc[sample_idx]['sequence_length']\n",
    "seq = needed_binding_sites_df.iloc[sample_idx]['sequence']\n",
    "seq_binding_sites = needed_binding_sites_df.iloc[sample_idx]['binding_sites']\n",
    "\n",
    "print(seq_label)\n",
    "print(seq)\n",
    "print(len(seq_label))\n",
    "print(seq_len)\n",
    "print(seq_binding_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set ligand distribution:\n",
      "ligand_type\n",
      "metal      3330\n",
      "small      1033\n",
      "nuclear     352\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set ligand distribution:\n",
      "ligand_type\n",
      "metal      833\n",
      "small      258\n",
      "nuclear     88\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set percentages:\n",
      "ligand_type\n",
      "metal      0.706257\n",
      "small      0.219088\n",
      "nuclear    0.074655\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set percentages:\n",
      "ligand_type\n",
      "metal      0.706531\n",
      "small      0.218830\n",
      "nuclear    0.074640\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    needed_binding_sites_df,\n",
    "    test_size=0.2,  # 80% train, 20% test\n",
    "    stratify=needed_binding_sites_df['ligand_type'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Verify the split ratios\n",
    "print(\"Training set ligand distribution:\")\n",
    "print(train_df['ligand_type'].value_counts())\n",
    "print(\"\\nTest set ligand distribution:\")\n",
    "print(test_df['ligand_type'].value_counts())\n",
    "\n",
    "# Calculate percentages to verify similar ratios\n",
    "print(\"\\nTraining set percentages:\")\n",
    "print(train_df['ligand_type'].value_counts(normalize=True))\n",
    "print(\"\\nTest set percentages:\")\n",
    "print(test_df['ligand_type'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1179.000000\n",
       "mean      373.797286\n",
       "std       285.582743\n",
       "min        37.000000\n",
       "25%       195.000000\n",
       "50%       301.000000\n",
       "75%       464.500000\n",
       "max      3391.000000\n",
       "Name: sequence_length, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sequence_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prot_id</th>\n",
       "      <th>binding_sites</th>\n",
       "      <th>ligand_type</th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>binary_binding_sites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>Q7CPA2</td>\n",
       "      <td>[99, 12, 13, 14, 84, 85, 87, 88, 89, 90]</td>\n",
       "      <td>metal</td>\n",
       "      <td>MLDVKSQDISIPEAVVVLCTAPDEATAQDLAAKVLAEKLAACATLL...</td>\n",
       "      <td>115</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>P0AB60</td>\n",
       "      <td>[360, 371, 357, 374]</td>\n",
       "      <td>metal</td>\n",
       "      <td>MLELLFLLLPVAAAYGWYMGRRSAQQNKQDEANRLSRDYVAGVNFL...</td>\n",
       "      <td>389</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>Q16611</td>\n",
       "      <td>[160, 164, 103, 104, 73, 170, 76, 46, 176, 177...</td>\n",
       "      <td>metal</td>\n",
       "      <td>MASGQGPGPPRQECGEPALPSASEEQVAQDTEEVFRSYVFYRHQQE...</td>\n",
       "      <td>211</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>Q9HYC5</td>\n",
       "      <td>[384, 258, 382, 262, 275, 181, 375, 280, 378, ...</td>\n",
       "      <td>metal</td>\n",
       "      <td>MTATSDLIESLISYSWDDWQVTRQEARRVIAAIRNDNVPDATIAAL...</td>\n",
       "      <td>408</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>P44007</td>\n",
       "      <td>[128, 58, 54]</td>\n",
       "      <td>metal</td>\n",
       "      <td>MPLLDSFKVDHTKMNAPAVRIAKTMLTPKGDNITVFDLRFCIPNKE...</td>\n",
       "      <td>167</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    prot_id                                      binding_sites ligand_type  \\\n",
       "633  Q7CPA2           [99, 12, 13, 14, 84, 85, 87, 88, 89, 90]       metal   \n",
       "507  P0AB60                               [360, 371, 357, 374]       metal   \n",
       "980  Q16611  [160, 164, 103, 104, 73, 170, 76, 46, 176, 177...       metal   \n",
       "648  Q9HYC5  [384, 258, 382, 262, 275, 181, 375, 280, 378, ...       metal   \n",
       "626  P44007                                      [128, 58, 54]       metal   \n",
       "\n",
       "                                              sequence  sequence_length  \\\n",
       "633  MLDVKSQDISIPEAVVVLCTAPDEATAQDLAAKVLAEKLAACATLL...              115   \n",
       "507  MLELLFLLLPVAAAYGWYMGRRSAQQNKQDEANRLSRDYVAGVNFL...              389   \n",
       "980  MASGQGPGPPRQECGEPALPSASEEQVAQDTEEVFRSYVFYRHQQE...              211   \n",
       "648  MTATSDLIESLISYSWDDWQVTRQEARRVIAAIRNDNVPDATIAAL...              408   \n",
       "626  MPLLDSFKVDHTKMNAPAVRIAKTMLTPKGDNITVFDLRFCIPNKE...              167   \n",
       "\n",
       "                                  binary_binding_sites  \n",
       "633  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "507  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "980  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "648  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "626  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split sequences into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(sequences, labels, chunk_size = 1000):\n",
    "    \"\"\"Split sequences and labels into chunks of size \"chunk_size\" or less.\"\"\"\n",
    "    new_sequences = []\n",
    "    new_labels = []\n",
    "    for seq, lbl in zip(sequences, labels):\n",
    "        if len(seq) > chunk_size:\n",
    "            # Split the sequence and labels into chunks of size \"chunk_size\" or less\n",
    "            for i in range(0, len(seq), chunk_size):\n",
    "                new_sequences.append(seq[i:i+chunk_size])\n",
    "                new_labels.append(lbl[i:i+chunk_size])\n",
    "        else:\n",
    "            new_sequences.append(seq)\n",
    "            new_labels.append(lbl)\n",
    "\n",
    "    return new_sequences, new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and testing sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial sequences\n",
    "test_seq = test_df['sequence'].tolist()\n",
    "test_labels = test_df['binary_binding_sites'].tolist()\n",
    "train_seq = train_df['sequence'].tolist()\n",
    "train_labels = train_df['binary_binding_sites'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply new sequences by chunking\n",
    "chunk_size = 1000\n",
    "test_seq, test_labels = split_into_chunks(test_seq, test_labels, chunk_size)\n",
    "train_seq, train_labels = split_into_chunks(train_seq, train_labels, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473\n",
      "473\n"
     ]
    }
   ],
   "source": [
    "print(len(train_seq[10]))\n",
    "print(len(train_labels[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "max_sequence_length = 1000 # here note that data was processed with chunks (context window) of 1000 residues - adapt accordingly\n",
    "\n",
    "train_tokenized = tokenizer(train_seq, padding=True, truncation=True, max_length=max_sequence_length, return_tensors=\"pt\", is_split_into_words=False)\n",
    "test_tokenized = tokenizer(test_seq, padding=True, truncation=True, max_length=max_sequence_length, return_tensors=\"pt\", is_split_into_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 20,  4,  ...,  1,  1,  1],\n",
       "        [ 0, 20,  4,  ...,  1,  1,  1],\n",
       "        [ 0, 20,  5,  ...,  1,  1,  1],\n",
       "        ...,\n",
       "        [ 0, 20,  9,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 17,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 20,  ...,  1,  1,  1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_labels(labels, max_length):\n",
    "    \"\"\"Truncate labels to the specified max_length.\"\"\"\n",
    "    return [label[:max_length] for label in labels]\n",
    "\n",
    "def compute_metrics_train(p):\n",
    "    \"\"\"Compute metrics for evaluation.\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove padding (-100 labels)\n",
    "    predictions = predictions[labels != -100].flatten()\n",
    "    labels = labels[labels != -100].flatten()\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    # Compute precision, recall, F1 score, and AUC\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    auc = roc_auc_score(labels, predictions)\n",
    "\n",
    "    # Compute MCC\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc, 'mcc': mcc}\n",
    "\n",
    "def compute_loss(model, inputs):\n",
    "    \"\"\"Custom compute_loss function.\"\"\"\n",
    "    logits = model(**inputs).logits\n",
    "    labels = inputs[\"labels\"]\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    active_loss = inputs[\"attention_mask\"].view(-1) == 1\n",
    "    active_logits = logits.view(-1, model.config.num_labels)\n",
    "    active_labels = torch.where(\n",
    "        active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "    )\n",
    "    loss = loss_fct(active_logits, active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "MRGETLKLKKDKRREAIRQQIDSNPFITDHELSDLFQVSIQTIRLDRTYLNIPELRKRIKLVAEKNYDQISSIEEQEFIGDLIQVNPNVKAQSILDITSDSVFHKTGIARGHVLFAQANSLCVALIKQPTVLTHESSIQFIEKVKLNDTVRAEARVVNQTAKHYYVEVKSYVKHTLVFKGNFKMFYDKRG\n",
      "{'input_ids': [0, 20, 10, 6, 9, 11, 4, 15, 4, 15, 15, 13, 15, 10, 10, 9, 5, 12, 10, 16, 16, 12, 13, 8, 17, 14, 18, 12, 11, 13, 21, 9, 4, 8, 13, 4, 18, 16, 7, 8, 12, 16, 11, 12, 10, 4, 13, 10, 11, 19, 4, 17, 12, 14, 9, 4, 10, 15, 10, 12, 15, 4, 7, 5, 9, 15, 17, 19, 13, 16, 12, 8, 8, 12, 9, 9, 16, 9, 18, 12, 6, 13, 4, 12, 16, 7, 17, 14, 17, 7, 15, 5, 16, 8, 12, 4, 13, 12, 11, 8, 13, 8, 7, 18, 21, 15, 11, 6, 12, 5, 10, 6, 21, 7, 4, 18, 5, 16, 5, 17, 8, 4, 23, 7, 5, 4, 12, 15, 16, 14, 11, 7, 4, 11, 21, 9, 8, 8, 12, 16, 18, 12, 9, 15, 7, 15, 4, 17, 13, 11, 7, 10, 5, 9, 5, 10, 7, 7, 17, 16, 11, 5, 15, 21, 19, 19, 7, 9, 7, 15, 8, 19, 7, 15, 21, 11, 4, 7, 18, 15, 6, 17, 18, 15, 20, 18, 19, 13, 15, 10, 6, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "<cls> M R G E T L K L K K D K R R E A I R Q Q I D S N P F I T D H E L S D L F Q V S I Q T I R L D R T Y L N I P E L R K R I K L V A E K N Y D Q I S S I E E Q E F I G D L I Q V N P N V K A Q S I L D I T S D S V F H K T G I A R G H V L F A Q A N S L C V A L I K Q P T V L T H E S S I Q F I E K V K L N D T V R A E A R V V N Q T A K H Y Y V E V K S Y V K H T L V F K G N F K M F Y D K R G <eos>\n",
      "190 192\n"
     ]
    }
   ],
   "source": [
    "# Verify some encoded and dedcode input, output\n",
    "idx = 100\n",
    "print(len(train_seq[idx]))\n",
    "print(train_seq[idx])\n",
    "encoded_input = tokenizer(train_seq[idx])\n",
    "print(encoded_input)\n",
    "decoded_output = tokenizer.decode(encoded_input['input_ids'])\n",
    "print(decoded_output)\n",
    "\n",
    "print(len(train_seq[idx]), len(encoded_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = truncate_labels(train_labels, max_sequence_length)\n",
    "test_labels = truncate_labels(test_labels, max_sequence_length)\n",
    "\n",
    "train_dataset = Dataset.from_dict({k: v for k, v in train_tokenized.items()}).add_column(\"labels\", train_labels)\n",
    "test_dataset = Dataset.from_dict({k: v for k, v in test_tokenized.items()}).add_column(\"labels\", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/esm2_t6_8M_UR50D\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/trained_models/esm2_t6_8M-binding-sites_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtimestamp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/trained_models/esm2_t6_8M-binding-sites_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtimestamp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable wandb\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/protein-embed/lib/python3.9/site-packages/transformers/training_args.py:1772\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1772\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[0;32m~/miniconda3/envs/protein-embed/lib/python3.9/site-packages/transformers/training_args.py:2294\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2293\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/protein-embed/lib/python3.9/site-packages/transformers/utils/generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/miniconda3/envs/protein-embed/lib/python3.9/site-packages/transformers/training_args.py:2167\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2168\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2169\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2170\u001b[0m         )\n\u001b[1;32m   2171\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2172\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"data/trained_models/esm2_t6_8M-binding-sites_{timestamp}\",\n",
    "    seed=42,\n",
    "    num_train_epochs = 3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=f\"data/trained_models/esm2_t6_8M-binding-sites_{timestamp}\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=5,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"  # Disable wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics_train,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
